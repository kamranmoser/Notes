Here’s a clean, copy-paste–ready version of the meeting notes you can drop directly into an email. I’ve kept it professional, structured, and aligned with how governance / dev audiences expect notes to look.

⸻

Subject: DQ App Working Session – Notes & Next Steps

Hi everyone,

Below are the meeting notes and takeaways from our recent Data Quality App working session.

⸻

Meeting Overview

Attendees: Kamran, Tashu, Saurabh, Huda
Purpose:
	•	Align on the current PARCM calculation process
	•	Review governance feedback
	•	Discuss future-state architecture and required backend work for automation, reporting, and governance integration

⸻

Key Discussion Points

1. Governance Alignment on PARCM Process
	•	Governance team reviewed the current PARCM calculation approach (Streamlit-based).
	•	Focus was on how control-level results roll up into metric-level PARCM results, not just the final percentages.
	•	Governance is aligned with:
	•	Submitting metric numbers directly via the application
	•	Using the application as evidence for how metrics were calculated for a given domain/month
	•	Important distinction confirmed:
	•	Metric numbers can be automated
	•	Failure commentary / remediation details must still be reviewed manually before submission (per Michael’s guidance)

⸻

2. PARCM Submission Model (Current + Future State)
	•	Today’s process includes two submissions:
	1.	Metric numbers
	2.	Failure details / remediation commentary (only required when there is a breach)
	•	Future-state vision:
	•	Application stores:
	•	Raw results
	•	Exclusions / inclusions
	•	Analyst comments
	•	Enables historical lookback (e.g., “how was last month’s metric calculated?”)
	•	Long-term goal: direct submission to ORCIT via API
	•	SharePoint remains a temporary step until full automation is live.

⸻

3. Daily Domain-Level Failure Emails
	•	Request from Vedad:
	•	Daily automated emails per domain
	•	Each email should include:
	•	High-level metric summary (pass/total)
	•	Categorized failures (SLA, GVR, nulls, dupes, APIs)
	•	Proposal:
	•	Leverage existing TechADS email framework (similar to current failure alerts)
	•	Pull data from Control Schema (not raw TechADS logs) so analyst context is included
	•	Open question to confirm:
	•	Whether emails should reflect:
	•	Day-before reviewed failures (with RCA), or
	•	Near-real-time snapshot (Tableau already serves this purpose)

⸻

4. Backend & SQL Work Required

Significant backend effort identified for the next phase:
	•	Three major SQL efforts required:
	1.	SQL for daily domain emails
	2.	SQL for Autosys ingestion into control tables
	3.	SQL for application UI views
	•	Schema updates discussed:
	•	Move from domain → target_category
	•	Remove analyst-only columns where not needed
	•	EIQ handling:
	•	Need to pull both pass and fail records to support total counts
	•	Introduce run_status / control_status instead of a strict failure ID
	•	Use run_id as the primary identifier
	•	Agreement to:
	•	Store SQL in DB where possible to avoid app redeploys for query-only changes
	•	Align with existing object-pull / DAO patterns where appropriate

⸻

5. Roles & Support
	•	Lakshmi to assist with:
	•	Control completion-time analysis
	•	SQL validation support
	•	Sridharshan & Amruta
	•	Focus first on onboarding + daily control monitoring
	•	Later assist with:
	•	Pre-release / post-release validation
	•	Verification tasks (not deep SQL development yet)

⸻

Action Items
	•	Kamran
	•	Update scripts to use target_category
	•	Finalize table changes and insert test data (month-level)
	•	Confirm daily email expectations with Vedad
	•	Backend / Dev
	•	Build and test required SQL scripts
	•	Validate schema changes in dev
	•	Team
	•	Schedule follow-up working session to review SQL + schema designs
	•	Align on EIQ table structure and failure handling logic

⸻

Please let me know if anything was missed or needs clarification. We’ll follow up with a working session to continue backend and automation alignment.

Thanks,
Kamran